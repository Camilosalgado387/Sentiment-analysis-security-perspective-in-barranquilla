{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re \n",
    "import pandas as pd\n",
    "import tweepy \n",
    "from tweepy import OAuthHandler \n",
    "from textblob import TextBlob \n",
    "import csv\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "n_words= set(stopwords.words('spanish'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "porter = PorterStemmer()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>X authentication</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TwitterClient(): \n",
    "      # keys and tokens from the Twitter Dev Console \n",
    "      consumer_key = 'xxxxxxxx'\n",
    "      consumer_secret =\"xxxxxxxxxxx\"\n",
    "      access_token = 'xxxxxxxxxx'\n",
    "      access_token_secret = 'xxxxxxxxxxx'\n",
    "\n",
    "      # attempt authentication \n",
    "      try: \n",
    "          # create OAuthHandler object \n",
    "          auth = OAuthHandler(consumer_key, consumer_secret) \n",
    "          # set access token and secret \n",
    "          auth.set_access_token(access_token, access_token_secret) \n",
    "          # create tweepy API object to fetch tweets \n",
    "          api = tweepy.API(auth) \n",
    "      except: \n",
    "          print(\"Error: Authentication Failed\") \n",
    "\n",
    "      return api #now we can make request to twitter using this api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocessing text(cleaning)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "  # removing @ tags and links from the text\n",
    "  text= ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) |(\\w+:\\/\\/\\S+)\", \" \", text).split()) \n",
    "  # converting all letters to lower case and relacing '-' with spaces.\n",
    "  text= text.lower().replace('-', ' ')\n",
    "  # removing stowards and numbers\n",
    "  table= str.maketrans('', '', string.punctuation+string.digits)\n",
    "  text= text.translate(table)\n",
    "  # tokenizing words \n",
    "  tokens = word_tokenize(text)\n",
    "  # stemming the words \n",
    "  stemmed = [porter.stem(word) for word in tokens]\n",
    "  words = [w for w in stemmed if not w in n_words]\n",
    "\n",
    "  text = ' '.join(words)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Analyzing sentiment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = TextBlob(tweet)\n",
    "senti= analysis.sentiment.polarity\n",
    "\n",
    "if senti<0 :\n",
    "  emotion = \"NEG\"\n",
    "elif senti>0:\n",
    "  emotion= \"POS\"\n",
    "else:\n",
    "  emotion= \"NEU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Getting Tweets</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(query, count = 10): \n",
    "      tweets = [] \n",
    "      try: \n",
    "          # call twitter api to fetch tweets \n",
    "          fetched_tweets = api.search(q = query, count = count) \n",
    "          for tweet in fetched_tweets:\n",
    "            # cleaning the tweets\n",
    "            tweet= clean(tweet.text)\n",
    "            # getting the sentiment from textblob\n",
    "            analysis = TextBlob(tweet)\n",
    "            senti= analysis.sentiment.polarity\n",
    "            # labeling the sentiment\n",
    "            if senti<0 :\n",
    "              emotion = \"NEG\"\n",
    "            elif senti>0:\n",
    "              emotion= \"POS\"\n",
    "            else:\n",
    "              emotion= \"NEU\"\n",
    "            # appending all data\n",
    "            tweets.append((tweet, senti, emotion))\n",
    "            \n",
    "          return tweets\n",
    "      except tweepy.TweepError as e:\n",
    "          # print error (if any) \n",
    "          print(\"Error : \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the api access\n",
    "api = TwitterClient() \n",
    "# calling function to get tweets, count is the number of tweets.\n",
    "tweets = get_tweets(query = \"Farmer's Protest\", count = 200)\n",
    "df= pd.DataFrame(tweets, columns= ['tweets', 'senti', 'emotion'])\n",
    "# droping retweets\n",
    "df= df.drop_duplicates()\n",
    "\n",
    "\n",
    "#save dataset in local\n",
    "df.to_csv('tweets.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
